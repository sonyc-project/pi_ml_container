{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import resampy\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(filepath, suffix, output_dir=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to audio file to be processed\n",
    "    suffix : str\n",
    "        String to append to filename (including extension)\n",
    "    output_dir : str or None\n",
    "        Path to directory where file will be saved. If None, will use directory of given filepath.\n",
    "    Returns\n",
    "    -------\n",
    "    output_path : str\n",
    "        Path to output file\n",
    "    \"\"\"\n",
    "    base_filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    if not output_dir:\n",
    "        output_dir = os.path.dirname(filepath)\n",
    "\n",
    "    if suffix[0] != '.':\n",
    "        output_filename = \"{}_{}\".format(base_filename, suffix)\n",
    "    else:\n",
    "        output_filename = base_filename + suffix\n",
    "\n",
    "    return os.path.join(output_dir, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _center_audio(audio, frame_len):\n",
    "    \"\"\"Center audio so that first sample will occur in the middle of the first frame\"\"\"\n",
    "    return np.pad(audio, (int(frame_len / 2.0), 0), mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "def _pad_audio(audio, frame_len, hop_len):\n",
    "    \"\"\"Pad audio if necessary so that all samples are processed\"\"\"\n",
    "    audio_len = audio.size\n",
    "    if audio_len < frame_len:\n",
    "        pad_length = frame_len - audio_len\n",
    "    else:\n",
    "        pad_length = int(np.ceil((audio_len - frame_len)/float(hop_len))) * hop_len \\\n",
    "                     - (audio_len - frame_len)\n",
    "\n",
    "    if pad_length > 0:\n",
    "        audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_softmax_from_tflite(audio, sr, model_path=None, hop_size=0.1, center=True,\\\n",
    "                            n_fft=None, n_mels=None, mel_hop_len=None, fmax=None):\n",
    "    \"\"\"\n",
    "    Computes and returns L3 embedding for given audio data\n",
    "    \"\"\"\n",
    "    if model_path is None:\n",
    "        raise ValueError('Tflite Model Path is missing')\n",
    "    \n",
    "    if audio.size == 0:\n",
    "        raise ValueError('Got empty audio')\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sr != TARGET_SR:\n",
    "        audio = resampy.resample(audio, sr_orig=sr, sr_new=TARGET_SR, filter='kaiser_best')\n",
    "\n",
    "    audio_len = audio.size\n",
    "    frame_len = TARGET_SR\n",
    "    hop_len = int(hop_size * TARGET_SR)\n",
    "\n",
    "    if audio_len < frame_len:\n",
    "        warnings.warn('Duration of provided audio is shorter than window size (1 second). Audio will be padded.',\n",
    "                      L3Warning)\n",
    "\n",
    "    if center:\n",
    "        # Center audio\n",
    "        audio = _center_audio(audio, frame_len)\n",
    "\n",
    "    # Pad if necessary to ensure that we process all samples\n",
    "    audio = _pad_audio(audio, frame_len, hop_len)\n",
    "\n",
    "    # Split audio into frames, copied from librosa.util.frame\n",
    "    frames = librosa.util.utils.frame(audio, frame_length=frame_len, hop_length=hop_len).T\n",
    "    X = []\n",
    "    for frame in frames:\n",
    "        S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_len,\\\n",
    "                                     window='hann', center=True, pad_mode='constant'))\n",
    "        S = librosa.feature.melspectrogram(sr=sr, S=S, n_mels=n_mels, fmax=fmax,\n",
    "                                           power=1.0, htk=True)\n",
    "        S = _amplitude_to_db(np.array(S))\n",
    "        X.append(S)\n",
    "\n",
    "    # Get the output for each frame\n",
    "    batch_size = min(len(X), 64)\n",
    "\n",
    "    predictions = []\n",
    "    interpreter = tf.lite.Interpreter(model_path=MODEL_PATH) \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "\n",
    "    #Reshape the tensor so that tflite can predict in batches\n",
    "    interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "   \n",
    "    #Predict for one audio file i.e. for one batch\n",
    "    x = np.array(X[:batch_size])[:, :, :, np.newaxis].astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, x)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_index)\n",
    "    predictions.append(output)\n",
    "\n",
    "#     predictions per batch in the absence of reshape   \n",
    "#     for idx, batch_x in enumerate(batch_size):\n",
    "#         x = np.array(X[idx])[np.newaxis, :, :, np.newaxis].astype(np.float32)\n",
    "#         interpreter.set_tensor(input_index, x)\n",
    "#         interpreter.invoke()\n",
    "#         output = interpreter.get_tensor(output_index)\n",
    "#         predictions.append(output)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath, output_dir=None, model_path=None, hop_size=0.1,\\\n",
    "                 n_fft=None, n_mels=None, mel_hop_len=None, fmax=None):\n",
    "    \"\"\"\n",
    "    Computes and saves L3 embedding for given audio file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise ValueError('File \"{}\" could not be found.'.format(filepath))\n",
    "\n",
    "    try:\n",
    "        audio, sr = sf.read(filepath)\n",
    "    except Exception:\n",
    "        raise ValueError('Could not open file \"{}\":\\n{}'.format(filepath, traceback.format_exc()))\n",
    "\n",
    "    output_path = get_output_path(filepath, \".npz\", output_dir=output_dir)\n",
    "\n",
    "    output = get_softmax_from_tflite(audio, sr, model_path=model_path, hop_size=hop_size,\\\n",
    "                                     n_fft=n_fft, n_mels=n_mels, mel_hop_len=mel_hop_len, fmax=fmax)\n",
    "\n",
    "    #coarse classes of sonyc = 8\n",
    "    pred = np.array(output).reshape(-1, 8)\n",
    "    #Get the position (or class) corresponding to the maximum output\n",
    "    pred_max = np.argmax(pred, axis=1)\n",
    "    \n",
    "    np.savez(output_path, label=pred_max)\n",
    "    assert os.path.exists(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 7 7 7 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 6\n",
      " 6 6 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 6 0 6 6 0 6 6 0]\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    TEST_DIR = os.path.dirname(os.path.realpath('__file__')) #os.path.dirname(__file__)\n",
    "    TEST_AUDIO_DIR = os.path.join(TEST_DIR, 'data/8k')\n",
    "    TFLITE_MODELS_DIR = os.path.join(TEST_DIR, 'tflite_models')\n",
    "    OUTPUT_DIR = os.path.join(TEST_DIR, 'output/sonyc_labels')\n",
    "    \n",
    "    MODEL_PATH = os.path.join(TFLITE_MODELS_DIR, 'full_quantized_default_float32.tflite')\n",
    "    SAMP_8K_PATH_1 = os.path.join(TEST_AUDIO_DIR, '08_003165.wav')\n",
    "    SAMP_8K_PATH_2 = os.path.join(TEST_AUDIO_DIR, '34_000997.wav')\n",
    "\n",
    "    if not os.path.isdir(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "    TARGET_SR = 8000\n",
    "    n_mels = 64\n",
    "    hop_size = 0.1 \n",
    "    mel_hop_len = 160\n",
    "    n_fft = 1024\n",
    "    fmax = None\n",
    "    \n",
    "    process_file(SAMP_8K_PATH_1, output_dir=OUTPUT_DIR, model_path=MODEL_PATH,\\\n",
    "                 hop_size=hop_size, n_mels=n_mels, n_fft=n_fft, mel_hop_len=mel_hop_len,\\\n",
    "                 fmax=fmax)\n",
    "    \n",
    "    # Labels are genrated for each frame. You can read the labels \n",
    "    labels = np.load(os.path.join(OUTPUT_DIR, '08_003165.npz'))\n",
    "    assert labels['label'].shape[0] == 64\n",
    "    print(labels['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
