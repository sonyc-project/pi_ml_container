{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import resampy\n",
    "from log import *\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger('quantized_inference')\n",
    "LOGGER.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr):\n",
    "    \"\"\"\n",
    "    Load audio file\n",
    "    \"\"\"\n",
    "    data, sr_orig = sf.read(path, dtype='float32', always_2d=True)\n",
    "    data = data.mean(axis=-1)\n",
    "\n",
    "    if sr_orig != sr:\n",
    "        data = resampy.resample(data, sr_orig, sr)\n",
    "\n",
    "    return data\n",
    "\n",
    "def amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec\n",
    "\n",
    "def get_melspectrogram(frame, n_fft=2048, mel_hop_length=242, samp_rate=48000, n_mels=256, fmax=None):\n",
    "    S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length, window='hann', center=True, pad_mode='constant'))\n",
    "    S = librosa.feature.melspectrogram(sr=samp_rate, S=S, n_fft=n_fft, n_mels=n_mels, fmax=fmax, power=1.0, htk=True)\n",
    "    S = amplitude_to_db(np.array(S))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_variables(sess):\n",
    "    if hasattr(tf, 'global_variables'):\n",
    "        variables = tf.global_variables()\n",
    "    else:\n",
    "        variables = tf.all_variables()\n",
    "\n",
    "    #print(variables)\n",
    "    uninitialized_variables = []\n",
    "    for v in variables:\n",
    "        if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n",
    "            uninitialized_variables.append(v)\n",
    "            v._keras_initialized = True\n",
    "    \n",
    "    #print(uninitialized_variables)\n",
    "    if uninitialized_variables:\n",
    "        if hasattr(tf, 'variables_initializer'):\n",
    "            sess.run(tf.variables_initializer(uninitialized_variables))\n",
    "        else:\n",
    "            sess.run(tf.initialize_variables(uninitialized_variables)) \n",
    "            \n",
    "def get_l3model(model_path, saved_model_type='tflite'):\n",
    "    l3embedding_model = tf.lite.Interpreter(model_path=model_path)  \n",
    "    return l3embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_us8k_metadata(path):\n",
    "    \"\"\"\n",
    "    Load UrbanSound8K metadata\n",
    "    Args:\n",
    "        path: Path to metadata csv file\n",
    "              (Type: str)\n",
    "    Returns:\n",
    "        metadata: List of metadata dictionaries\n",
    "                  (Type: list[dict[str, *]])\n",
    "    \"\"\"\n",
    "    metadata = [{} for _ in range(10)]\n",
    "    with open(path) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            fname = row['slice_file_name']\n",
    "            row['start'] = float(row['start'])\n",
    "            row['end'] = float(row['end'])\n",
    "            row['salience'] = float(row['salience'])\n",
    "            fold_num = row['fold'] = int(row['fold'])\n",
    "            row['classID'] = int(row['classID'])\n",
    "            metadata[fold_num-1][fname] = row\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def get_l3_frames_uniform_tflite(audio, interpreter=None, n_fft=2048, n_mels=256,\\\n",
    "                                 mel_hop_length=242, hop_size=0.1, sr=48000, fmax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Get L3 embedding from tflite model for each frame in the given audio file\n",
    "\n",
    "    Args:\n",
    "        audio: Audio data or path to audio file\n",
    "               (Type: np.ndarray or str)\n",
    "\n",
    "        l3embedding_model:  Audio embedding model\n",
    "                            (keras.engine.training.Model)\n",
    "\n",
    "    Keyword Args:\n",
    "        hop_size: Hop size in seconds\n",
    "                  (Type: float)\n",
    "\n",
    "    Returns:\n",
    "        features:  Array of embedding vectors\n",
    "                   (Type: np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(audio) == str:\n",
    "        audio = load_audio(audio, sr)\n",
    "\n",
    "    hop_size = hop_size\n",
    "    hop_length = int(hop_size * sr)\n",
    "    frame_length = sr * 1\n",
    "\n",
    "    audio_length = len(audio)\n",
    "    if audio_length < frame_length:\n",
    "        # Make sure we can have at least one frame of audio\n",
    "        pad_length = frame_length - audio_length\n",
    "    else:\n",
    "        # Zero pad so we compute embedding on all samples\n",
    "        pad_length = int(np.ceil(audio_length - frame_length)/hop_length) * hop_length \\\n",
    "                     - (audio_length - frame_length)\n",
    "\n",
    "    if pad_length > 0:\n",
    "        # Use (roughly) symmetric padding\n",
    "        left_pad = pad_length // 2\n",
    "        right_pad= pad_length - left_pad\n",
    "        audio = np.pad(audio, (left_pad, right_pad), mode='constant')\n",
    "   \n",
    "    frames = librosa.util.utils.frame(audio, frame_length=frame_length, hop_length=hop_length).T\n",
    "    X = []\n",
    "    for frame in frames:\n",
    "        S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_length,\\\n",
    "                                     window='hann', center=True,\\\n",
    "                                     pad_mode='constant'))\n",
    "        S = librosa.feature.melspectrogram(sr=sr, S=S, n_mels=n_mels, fmax=fmax,\n",
    "                                           power=1.0, htk=True)\n",
    "        S = amplitude_to_db(np.array(S))\n",
    "        X.append(S)\n",
    "\n",
    "    #X = np.array(X)[:, :, :, np.newaxis].astype(np.float32)\n",
    "\n",
    "    # Get the L3 embedding for each frame\n",
    "    batch_size = len(X)\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "    embedding_length = output_shape[-1]\n",
    "    \n",
    "    #interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    #interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(output_shape)))\n",
    "     \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "    \n",
    "    predictions = np.zeros((batch_size, embedding_length), dtype=np.float32)\n",
    "    for idx in range(len(X)):\n",
    "        #predictions per batch\n",
    "        #print(np.array(X[idx]).shape)\n",
    "        x = np.array(X[idx])[np.newaxis, :, :, np.newaxis].astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        #print('Interpreter Invoked!')\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        predictions[idx] = np.reshape(output, (output.shape[0], output.shape[-1]))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def compute_file_features(path, feature_type, l3embedding_model=None, model_type='keras', **feature_args):\n",
    "           \n",
    "    if feature_type == 'l3':\n",
    "        if not l3embedding_model:\n",
    "            err_msg = 'Must provide L3 embedding model to use {} features'\n",
    "            raise ValueError(err_msg.format(feature_type))\n",
    "\n",
    "        if model_type == 'tflite':\n",
    "            print('Computing embedding for: ', os.path.basename(path))\n",
    "            file_features = get_l3_frames_uniform_tflite(path, interpreter=l3embedding_model, **feature_args)\n",
    "        else:\n",
    "            raise ValueError('Only tflite models supported!')\n",
    "            \n",
    "    else:\n",
    "        raise ValueError('Invalid feature type: {}'.format(feature_type))\n",
    "\n",
    "    return file_features\n",
    "\n",
    "def generate_us8k_fold_data(metadata, data_dir, fold_idx, output_dir, l3embedding_model=None, model_type='keras',\n",
    "                            features='l3', random_state=12345678, **feature_args):\n",
    "    \"\"\"\n",
    "    Generate all of the data for a specific fold\n",
    "\n",
    "    Args:\n",
    "        metadata: List of metadata dictionaries, or a path to a metadata file to be loaded\n",
    "                  (Type: list[dict[str,*]] or str)\n",
    "\n",
    "        data_dir: Path to data directory\n",
    "                  (Type: str)\n",
    "\n",
    "        fold_idx: Index of fold to load\n",
    "                  (Type: int)\n",
    "\n",
    "        output_dir: Path to output directory where fold data will be stored\n",
    "                    (Type: str)\n",
    "\n",
    "    Keyword Args:\n",
    "        l3embedding_model: L3 embedding model, used if L3 features are used\n",
    "                           (Type: keras.engine.training.Model or None)\n",
    "\n",
    "        features: Type of features to be computed\n",
    "                  (Type: str)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if type(metadata) == str:\n",
    "        metadata = load_us8k_metadata(metadata)\n",
    "\n",
    "    # Set random seed\n",
    "    random_state = random_state + fold_idx\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    audio_fold_dir = os.path.join(data_dir, \"fold{}\".format(fold_idx+1))\n",
    "\n",
    "    # Create fold directory if it does not exist\n",
    "    output_fold_dir = os.path.join(output_dir, \"fold{}\".format(fold_idx+1))\n",
    "    if not os.path.isdir(output_fold_dir):\n",
    "        os.makedirs(output_fold_dir)\n",
    "\n",
    "    LOGGER.info('Generating fold {} in {}'.format(fold_idx+1, output_fold_dir))\n",
    "    print('Generating fold {} in {}'.format(fold_idx+1, output_fold_dir))\n",
    "    \n",
    "    num_files = len(metadata[fold_idx])\n",
    "\n",
    "    for idx, (fname, example_metadata) in enumerate(metadata[fold_idx].items()):\n",
    "        desc = '({}/{}) Processed {} -'.format(idx+1, num_files, fname)\n",
    "        with LogTimer(LOGGER, desc, log_level=logging.DEBUG):\n",
    "            # TODO: Make sure glob doesn't catch things with numbers afterwards\n",
    "            variants = [x for x in glob.glob(os.path.join(audio_fold_dir,\n",
    "                '**', os.path.splitext(fname)[0] + '[!0-9]*[wm][ap][v3]'), recursive=True)\n",
    "                if os.path.isfile(x) and not x.endswith('.jams')]\n",
    "            num_variants = len(variants)\n",
    "            for var_idx, var_path in enumerate(variants):\n",
    "                audio_dir = os.path.dirname(var_path)\n",
    "                var_fname = os.path.basename(var_path)\n",
    "                desc = '\\t({}/{}) Variants {} -'.format(var_idx+1, num_variants, var_fname)\n",
    "                with LogTimer(LOGGER, desc, log_level=logging.DEBUG):\n",
    "                    generate_us8k_file_data(var_fname, example_metadata, audio_dir,\n",
    "                                            output_fold_dir, features,\n",
    "                                            l3embedding_model, model_type, **feature_args)\n",
    "\n",
    "\n",
    "def generate_us8k_file_data(fname, example_metadata, audio_fold_dir,\n",
    "                            output_fold_dir, features,\n",
    "                            l3embedding_model, model_type, **feature_args):\n",
    "    audio_path = os.path.join(audio_fold_dir, fname)\n",
    "\n",
    "    basename, _ = os.path.splitext(fname)\n",
    "    output_path = os.path.join(output_fold_dir, basename + '.npz')\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        LOGGER.info('File {} already exists'.format(output_path))\n",
    "        return\n",
    "\n",
    "    print('Filename: ', fname)\n",
    "    X = compute_file_features(audio_path, features, l3embedding_model=l3embedding_model,\\\n",
    "                              model_type=model_type, **feature_args)\n",
    "\n",
    "    # If we were not able to compute the features, skip this file\n",
    "    if X is None:\n",
    "        LOGGER.error('Could not generate data for {}'.format(audio_path))\n",
    "        return\n",
    "\n",
    "    class_label = example_metadata['classID']\n",
    "    y = class_label\n",
    "\n",
    "    np.savez_compressed(output_path, X=X, y=y)\n",
    "    return output_path, 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating fold 1 in /scratch/sk7898/test_quant_tflite/fold1\n",
      "Filename:  101415-3-0-2.wav\n",
      "Computing embedding for:  101415-3-0-2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-cpu/lib/python3.6/site-packages/librosa/filters.py:284: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "{'name': 'input_13', 'index': 28, 'shape': array([  1, 256, 199,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'max_pooling2d_1/MaxPool', 'index': 29, 'shape': array([  1,   1,   1, 512], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "Filename:  101415-3-0-3.wav\n",
      "Computing embedding for:  101415-3-0-3.wav\n",
      "== Input details ==\n",
      "{'name': 'input_13', 'index': 28, 'shape': array([  1, 256, 199,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'max_pooling2d_1/MaxPool', 'index': 29, 'shape': array([  1,   1,   1, 512], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "Filename:  101415-3-0-8.wav\n",
      "Computing embedding for:  101415-3-0-8.wav\n",
      "== Input details ==\n",
      "{'name': 'input_13', 'index': 28, 'shape': array([  1, 256, 199,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'max_pooling2d_1/MaxPool', 'index': 29, 'shape': array([  1,   1,   1, 512], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-12ca869f5af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0mmel_hop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_hop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                             n_fft=n_dft, fmax=fmax, sr=samp_rate, with_melSpec=with_melSpec)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-83c75eed846d>\u001b[0m in \u001b[0;36mgenerate_us8k_fold_data\u001b[0;34m(metadata, data_dir, fold_idx, output_dir, l3embedding_model, model_type, features, random_state, **feature_args)\u001b[0m\n\u001b[1;32m    195\u001b[0m                     generate_us8k_file_data(var_fname, example_metadata, audio_dir,\n\u001b[1;32m    196\u001b[0m                                             \u001b[0moutput_fold_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                                             l3embedding_model, model_type, **feature_args)\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-83c75eed846d>\u001b[0m in \u001b[0;36mgenerate_us8k_file_data\u001b[0;34m(fname, example_metadata, audio_fold_dir, output_fold_dir, features, l3embedding_model, model_type, **feature_args)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Filename: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     X = compute_file_features(audio_path, features, l3embedding_model=l3embedding_model,\\\n\u001b[0;32m--> 214\u001b[0;31m                               model_type=model_type, **feature_args)\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# If we were not able to compute the features, skip this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-83c75eed846d>\u001b[0m in \u001b[0;36mcompute_file_features\u001b[0;34m(path, feature_type, l3embedding_model, model_type, **feature_args)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tflite'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Computing embedding for: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mfile_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_l3_frames_uniform_tflite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml3embedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfeature_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Only tflite models supported!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-83c75eed846d>\u001b[0m in \u001b[0;36mget_l3_frames_uniform_tflite\u001b[0;34m(audio, interpreter, n_fft, n_mels, mel_hop_length, hop_size, sr, fmax, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m#print('Interpreter Invoked!')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-cpu/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[1;32m    303\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sk7898/miniconda3/envs/l3embedding-tf-14-cpu/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\u001b[0m in \u001b[0;36mInvoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensorflow_wrap_interpreter_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreterWrapper_Invoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mInputIndices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    model_path = '/scratch/sk7898/quantization/l3_audio_original_48000_256_242_2048/quantized_model_size.tflite'\n",
    "    fold_num = 1\n",
    "    metadata_path = '/beegfs/jtc440/UrbanSound8K/metadata/UrbanSound8K.csv'\n",
    "    data_dir = '/beegfs/jtc440/UrbanSound8K/audio'\n",
    "    dataset_output_dir = '/scratch/sk7898/test_quant_tflite'\n",
    "    random_state = 20180302\n",
    "    samp_rate = 48000\n",
    "    n_mels = 256\n",
    "    n_hop = 242\n",
    "    n_dft = 2048 \n",
    "    fmax=None\n",
    "    with_melSpec = False\n",
    "    \n",
    "    saved_model_type = 'tflite' \n",
    "    l3embedding_model = get_l3model(model_path, saved_model_type=saved_model_type)\n",
    "\n",
    "    # Generate a single fold if a fold was specified\n",
    "    generate_us8k_fold_data(metadata_path, data_dir, fold_num-1, dataset_output_dir,\n",
    "                            l3embedding_model=l3embedding_model, model_type=saved_model_type, \n",
    "                            features='l3', random_state=random_state,\n",
    "                            mel_hop_length=n_hop, n_mels=n_mels,\\\n",
    "                            n_fft=n_dft, fmax=fmax, sr=samp_rate, with_melSpec=with_melSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 512)\n",
      "(31, 512)\n"
     ]
    }
   ],
   "source": [
    "# to be done post embedding generation\n",
    "dataset_output_dir = '/scratch/sk7898/test_quant_tflite/'\n",
    "embedding = np.load(os.path.join(dataset_output_dir, 'fold1/101415-3-0-2.npz'))\n",
    "assert embedding['X'].shape[1] == 512\n",
    "print(embedding['X'].shape)\n",
    "\n",
    "embedding = np.load(os.path.join(dataset_output_dir, 'fold1/101415-3-0-3.npz'))\n",
    "assert embedding['X'].shape[1] == 512\n",
    "print(embedding['X'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
