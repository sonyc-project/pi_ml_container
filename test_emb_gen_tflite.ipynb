{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import resampy\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_variables(sess):\n",
    "    if hasattr(tf, 'global_variables'):\n",
    "        variables = tf.global_variables()\n",
    "    else:\n",
    "        variables = tf.all_variables()\n",
    "\n",
    "    uninitialized_variables = []\n",
    "    for v in variables:\n",
    "        if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n",
    "            uninitialized_variables.append(v)\n",
    "            v._keras_initialized = True\n",
    "\n",
    "    if uninitialized_variables:\n",
    "        if hasattr(tf, 'variables_initializer'):\n",
    "            sess.run(tf.variables_initializer(uninitialized_variables))\n",
    "        else:\n",
    "            sess.run(tf.initialize_variables(uninitialized_variables)) \n",
    "            \n",
    "def get_l3model(model_path, saved_model_type='tflite'):\n",
    "    l3embedding_model = tf.lite.Interpreter(model_path=model_path)  \n",
    "    return l3embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(filepath, suffix, output_dir=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to audio file to be processed\n",
    "    suffix : str\n",
    "        String to append to filename (including extension)\n",
    "    output_dir : str or None\n",
    "        Path to directory where file will be saved. If None, will use directory of given filepath.\n",
    "    Returns\n",
    "    -------\n",
    "    output_path : str\n",
    "        Path to output file\n",
    "    \"\"\"\n",
    "    base_filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    if not output_dir:\n",
    "        output_dir = os.path.dirname(filepath)\n",
    "\n",
    "    if suffix[0] != '.':\n",
    "        output_filename = \"{}_{}\".format(base_filename, suffix)\n",
    "    else:\n",
    "        output_filename = base_filename + suffix\n",
    "\n",
    "    return os.path.join(output_dir, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _center_audio(audio, frame_len):\n",
    "    \"\"\"Center audio so that first sample will occur in the middle of the first frame\"\"\"\n",
    "    return np.pad(audio, (int(frame_len / 2.0), 0), mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "def _pad_audio(audio, frame_len, hop_len):\n",
    "    \"\"\"Pad audio if necessary so that all samples are processed\"\"\"\n",
    "    audio_len = audio.size\n",
    "    if audio_len < frame_len:\n",
    "        pad_length = frame_len - audio_len\n",
    "    else:\n",
    "        pad_length = int(np.ceil((audio_len - frame_len)/float(hop_len))) * hop_len \\\n",
    "                     - (audio_len - frame_len)\n",
    "\n",
    "    if pad_length > 0:\n",
    "        audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec\n",
    "\n",
    "def get_embedding(audio, sr, model=None, hop_size=0.1, center=True,\\\n",
    "                  n_fft=None, n_mels=None, mel_hop_len=None, fmax=None):\n",
    "    \"\"\"\n",
    "    Computes and returns L3 embedding for given audio data\n",
    "    \"\"\"\n",
    "    interpreter = model\n",
    "    \n",
    "    if audio.size == 0:\n",
    "        raise ValueError('Got empty audio')\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sr != TARGET_SR:\n",
    "        audio = resampy.resample(audio, sr_orig=sr, sr_new=TARGET_SR, filter='kaiser_best')\n",
    "\n",
    "    audio_len = audio.size\n",
    "    frame_len = TARGET_SR\n",
    "    hop_len = int(hop_size * TARGET_SR)\n",
    "\n",
    "    if audio_len < frame_len:\n",
    "        warnings.warn('Duration of provided audio is shorter than window size (1 second). Audio will be padded.',\n",
    "                      L3Warning)\n",
    "\n",
    "    if center:\n",
    "        # Center audio\n",
    "        audio = _center_audio(audio, frame_len)\n",
    "\n",
    "    # Pad if necessary to ensure that we process all samples\n",
    "    audio = _pad_audio(audio, frame_len, hop_len)\n",
    "\n",
    "    # Split audio into frames, copied from librosa.util.frame\n",
    "    frames = librosa.util.utils.frame(audio, frame_length=frame_len, hop_length=hop_len).T\n",
    "    X = []\n",
    "    for frame in frames:\n",
    "        S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_len,\\\n",
    "                                     window='hann', center=True, pad_mode='constant'))\n",
    "        S = librosa.feature.melspectrogram(sr=sr, S=S, n_mels=n_mels, fmax=fmax,\n",
    "                                           power=1.0, htk=True)\n",
    "        S = _amplitude_to_db(np.array(S))\n",
    "        X.append(S)\n",
    "\n",
    "    #X = np.array(X)[:, :, :, np.newaxis].astype(np.float32)\n",
    "\n",
    "    # Get the L3 embedding for each frame\n",
    "    batch_size = len(X)\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "    embedding_length = output_shape[-1]\n",
    "    \n",
    "    #interpreter.resize_tensor_input(input_index, ((batch_size, ) + tuple(input_shape)))\n",
    "    #interpreter.resize_tensor_input(output_index, ((batch_size, ) + tuple(output_shape)))\n",
    "     \n",
    "    print(\"== Input details ==\")\n",
    "    print(interpreter.get_input_details()[0])\n",
    "    print(\"type:\", input_details[0]['dtype'])\n",
    "    print(\"\\n== Output details ==\")\n",
    "    print(interpreter.get_output_details()[0])\n",
    "    \n",
    "    predictions = np.zeros((batch_size, embedding_length), dtype=np.float32)\n",
    "    for idx in range(len(X)):\n",
    "        #predictions per batch\n",
    "        #print(np.array(X[idx]).shape)\n",
    "        x = np.array(X[idx])[np.newaxis, :, :, np.newaxis].astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        #print('Interpreter Invoked!')\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        predictions[idx] = np.reshape(output, (output.shape[0], output.shape[-1]))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def process_file(filepath, output_dir=None, model=None, hop_size=0.1,\\\n",
    "                 n_fft=None, n_mels=None, mel_hop_len=None, fmax=None):\n",
    "    \"\"\"\n",
    "    Computes and saves L3 embedding for given audio file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise ValueError('File \"{}\" could not be found.'.format(filepath))\n",
    "\n",
    "    try:\n",
    "        audio, sr = sf.read(filepath)\n",
    "    except Exception:\n",
    "        raise ValueError('Could not open file \"{}\":\\n{}'.format(filepath, traceback.format_exc()))\n",
    "\n",
    "    output_path = get_output_path(filepath, \".npz\", output_dir=output_dir)\n",
    "\n",
    "    embedding = get_embedding(audio, sr, model=model, hop_size=hop_size,\\\n",
    "                              n_fft=n_fft, n_mels=n_mels, mel_hop_len=mel_hop_len, fmax=fmax)\n",
    "\n",
    "    np.savez(output_path, embedding=embedding)\n",
    "    assert os.path.exists(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "{'name': 'input_13', 'index': 28, 'shape': array([  1, 256, 199,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "{'name': 'max_pooling2d_1/MaxPool', 'index': 29, 'shape': array([  1,   1,   1, 512], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    TEST_DIR = os.path.dirname(os.path.realpath('__file__')) #os.path.dirname(__file__)\n",
    "    TEST_AUDIO_DIR = os.path.join(TEST_DIR, 'data')\n",
    "    TFLITE_MODELS_DIR = os.path.join(TEST_DIR, 'tflite_models')\n",
    "    OUTPUT_DIR = os.path.join(TEST_DIR, 'output')\n",
    "    \n",
    "    model_path = os.path.join(TFLITE_MODELS_DIR, 'quantized_model_size.tflite')\n",
    "    CHIRP_1S_PATH = os.path.join(TEST_AUDIO_DIR, 'chirp_1s.wav')\n",
    "    CHIRP_44K_PATH = os.path.join(TEST_AUDIO_DIR, 'chirp_44k.wav')\n",
    "\n",
    "    if not os.path.isdir(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        \n",
    "    TARGET_SR = 48000\n",
    "    n_mels = 256\n",
    "    hop_size = 0.1 \n",
    "    mel_hop_len = 242\n",
    "    n_fft = 2048 \n",
    "    fmax=None\n",
    "    \n",
    "    saved_model_type = 'tflite' \n",
    "    l3embedding_model = get_l3model(model_path, saved_model_type=saved_model_type)\n",
    "\n",
    "    process_file(CHIRP_1S_PATH, output_dir=OUTPUT_DIR, model=l3embedding_model, hop_size=hop_size,\\\n",
    "                 n_mels=n_mels, n_fft=n_fft, mel_hop_len=mel_hop_len, fmax=fmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42174116 1.533261   0.6907865  ... 3.2289128  2.6567705  2.5342178 ]\n",
      " [0.44129398 1.5515698  0.80050015 ... 3.3703237  2.7017074  2.5407126 ]\n",
      " [0.36863136 1.5548941  0.7092948  ... 3.2482824  2.6646929  2.5490541 ]\n",
      " [0.3982991  1.5480113  0.7412488  ... 3.3637795  2.7074559  2.5882218 ]\n",
      " [0.31669328 1.5234326  0.7273678  ... 3.1760914  2.674885   2.6374516 ]\n",
      " [0.34958306 1.5700997  0.7912759  ... 3.6013808  2.7372866  2.6660237 ]]\n",
      "(6, 512)\n"
     ]
    }
   ],
   "source": [
    "emb_chirp = np.load(os.path.join(OUTPUT_DIR, 'chirp_1s.npz'))\n",
    "print(emb_chirp['embedding'])\n",
    "print(emb_chirp['embedding'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
