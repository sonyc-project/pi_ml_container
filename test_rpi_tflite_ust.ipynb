{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import traceback\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import resampy\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from metrics import evaluate, micro_averaged_auprc, macro_averaged_auprc\n",
    "import oyaml as yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(filepath, suffix, output_dir=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to audio file to be processed\n",
    "    suffix : str\n",
    "        String to append to filename (including extension)\n",
    "    output_dir : str or None\n",
    "        Path to directory where file will be saved. If None, will use directory of given filepath.\n",
    "    Returns\n",
    "    -------\n",
    "    output_path : str\n",
    "        Path to output file\n",
    "    \"\"\"\n",
    "    base_filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    if not output_dir:\n",
    "        output_dir = os.path.dirname(filepath)\n",
    "\n",
    "    if suffix[0] != '.':\n",
    "        output_filename = \"{}_{}\".format(base_filename, suffix)\n",
    "    else:\n",
    "        output_filename = base_filename + suffix\n",
    "\n",
    "    return os.path.join(output_dir, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tflite(model_path):\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape'][1:]\n",
    "    output_shape = output_details[0]['shape'][1:]\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "    emb_len = output_shape[-1]\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    return interpreter, input_index, output_index, output_shape, emb_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _center_audio(audio, frame_len):\n",
    "    \"\"\"Center audio so that first sample will occur in the middle of the first frame\"\"\"\n",
    "    return np.pad(audio, (int(frame_len / 2.0), 0), mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "def _pad_audio(audio, frame_len, hop_len):\n",
    "    \"\"\"Pad audio if necessary so that all samples are processed\"\"\"\n",
    "    audio_len = audio.size\n",
    "    if audio_len < frame_len:\n",
    "        pad_length = frame_len - audio_len\n",
    "    else:\n",
    "        pad_length = int(np.ceil((audio_len - frame_len)/float(hop_len))) * hop_len \\\n",
    "                     - (audio_len - frame_len)\n",
    "\n",
    "    if pad_length > 0:\n",
    "        audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)\n",
    "\n",
    "    return audio\n",
    "\n",
    "def _amplitude_to_db(S, amin=1e-10, dynamic_range=80.0):\n",
    "    magnitude = np.abs(S)\n",
    "    power = np.square(magnitude, out=magnitude)\n",
    "    ref_value = power.max()\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= log_spec.max()\n",
    "\n",
    "    log_spec = np.maximum(log_spec, -dynamic_range)\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_upstream(audio, sr, \n",
    "                                 interpreter,\n",
    "                                 input_index=None,\n",
    "                                 output_index=None,\n",
    "                                 output_shape=None,\n",
    "                                 emb_len=256,\n",
    "                                 hop_size=0.1, \n",
    "                                 center=True,\n",
    "                                 n_fft=None, \n",
    "                                 n_mels=None, \n",
    "                                 mel_hop_len=None, \n",
    "                                 fmax=None):\n",
    "    \"\"\"\n",
    "    Computes and returns L3 embedding for given audio data\n",
    "    \"\"\"\n",
    "    \n",
    "    if not interpreter:\n",
    "        raise ValueError('Tflite Model is missing')\n",
    "    \n",
    "    if audio.size == 0:\n",
    "        raise ValueError('Got empty audio')\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sr != TARGET_SR:\n",
    "        audio = resampy.resample(audio, sr_orig=sr, sr_new=TARGET_SR, filter='kaiser_best')\n",
    "\n",
    "    audio_len = audio.size\n",
    "    frame_len = TARGET_SR\n",
    "    hop_len = int(hop_size * TARGET_SR)\n",
    "\n",
    "    if audio_len < frame_len:\n",
    "        warnings.warn('Duration of provided audio is shorter than window size (1 second). Audio will be padded.',\n",
    "                      L3Warning)\n",
    "\n",
    "    if center:\n",
    "        # Center audio\n",
    "        audio = _center_audio(audio, frame_len)\n",
    "\n",
    "    # Pad if necessary to ensure that we process all samples\n",
    "    audio = _pad_audio(audio, frame_len, hop_len)\n",
    "\n",
    "    # Split audio into frames, copied from librosa.util.frame\n",
    "    frames = librosa.util.utils.frame(audio, frame_length=frame_len, hop_length=hop_len).T\n",
    "    \n",
    "    X = []\n",
    "    for frame in frames:\n",
    "        S = np.abs(librosa.core.stft(frame, n_fft=n_fft, hop_length=mel_hop_len,\\\n",
    "                                     window='hann', center=True, pad_mode='constant'))\n",
    "        S = librosa.feature.melspectrogram(sr=sr, S=S, n_mels=n_mels, fmax=fmax,\n",
    "                                           power=1.0, htk=True)\n",
    "        S = _amplitude_to_db(np.array(S))\n",
    "        X.append(S)\n",
    "\n",
    "        \n",
    "    predictions = []\n",
    "    \n",
    "    #embeddings per frame   \n",
    "    for idx in range(len(X)):\n",
    "        x = np.array(X[idx])[np.newaxis, :, :, np.newaxis].astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_index)\n",
    "        predictions.append(output)\n",
    "    \n",
    "    predictions = np.array(predictions).reshape(-1, emb_len)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_from_classifier(X,\n",
    "                               classifier,\n",
    "                               input_index=None,\n",
    "                               output_index=None,\n",
    "                               output_shape=None,\n",
    "                               n_classes=8):\n",
    "    \"\"\"\n",
    "    Predicts the softmax output from the embeddings extracted ffrom upstream model\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    #softmax per frame   \n",
    "    for idx in range(len(X)):\n",
    "        x = np.array(X[idx])[np.newaxis, :].astype(np.float32)\n",
    "        classifier.set_tensor(input_index, x)\n",
    "        classifier.invoke()\n",
    "        output = classifier.get_tensor(output_index)\n",
    "        predictions.append(output)\n",
    "    \n",
    "    predictions = np.array(predictions).reshape(-1, n_classes)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_output(output_path, test_file_list, y_pred, taxonomy):\n",
    "    \n",
    "    coarse_fine_labels = [[\"{}-{}_{}\".format(coarse_id, fine_id, fine_label)\n",
    "                           for fine_id, fine_label in fine_dict.items()]\n",
    "                          for coarse_id, fine_dict in taxonomy['fine'].items()]\n",
    "        \n",
    "    full_fine_target_labels = [fine_label for fine_list in coarse_fine_labels\n",
    "                               for fine_label in fine_list]\n",
    "        \n",
    "    coarse_target_labels = [\"_\".join([str(k), v])\n",
    "                            for k, v in taxonomy['coarse'].items()]\n",
    "        \n",
    "    with open(output_path, 'w') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "\n",
    "        # Write fields\n",
    "        fields = [\"audio_filename\"] + full_fine_target_labels + coarse_target_labels\n",
    "        csvwriter.writerow(fields)\n",
    "\n",
    "        # Write results for each file to CSV\n",
    "        for filename, y, in zip(test_file_list, y_pred):\n",
    "            row = [filename]\n",
    "\n",
    "            # Add placeholder values for fine level\n",
    "            row += [0.0 for _ in range(len(full_fine_target_labels))]\n",
    "            # Add coarse level labels\n",
    "            row += list(y)\n",
    "\n",
    "            csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(file_list, \n",
    "                  taxonomy, \n",
    "                  output_path, \n",
    "                  upstream_path=None, \n",
    "                  classifier_path=None,\n",
    "                  hop_size=0.1,\n",
    "                  n_fft=None, \n",
    "                  n_mels=None, \n",
    "                  mel_hop_len=None, \n",
    "                  fmax=None):\n",
    "    \"\"\"\n",
    "    Computes and saves L3 embedding for audio files\n",
    "    \"\"\"\n",
    "    y_pred_mean = []\n",
    "    interpreter, input_index, output_index, output_shape, emb_len = get_tflite(upstream_path)\n",
    "    classifier, cls_input_index, cls_output_index, cls_output_shape, _ = get_tflite(classifier_path)\n",
    "        \n",
    "    for filepath in file_list:\n",
    "        try:\n",
    "            audio, sr = sf.read(filepath)\n",
    "        except Exception:\n",
    "            raise ValueError('Could not open file \"{}\":\\n{}'.format(filepath, traceback.format_exc()))\n",
    "        \n",
    "        # Embeddings output per frame \n",
    "        # Shape: (number_of_frames, emb_len)\n",
    "        embeddings = get_embeddings_from_upstream(audio, sr,\n",
    "                                                  interpreter,\n",
    "                                                  input_index=input_index,\n",
    "                                                  output_index=output_index,\n",
    "                                                  output_shape=output_shape,\n",
    "                                                  emb_len=emb_len,\n",
    "                                                  hop_size=hop_size,\n",
    "                                                  n_fft=n_fft,\n",
    "                                                  n_mels=n_mels,\n",
    "                                                  mel_hop_len=mel_hop_len,\n",
    "                                                  fmax=fmax)\n",
    "\n",
    "        if embeddings is None:\n",
    "            LOGGER.error('Could not generate embedding for {}'.format(filepath))\n",
    "            return\n",
    "        \n",
    "        # Softmax output per frame \n",
    "        # Shape: (number_of_frames, 8)\n",
    "        output = get_output_from_classifier(embeddings,\n",
    "                                            classifier,\n",
    "                                            input_index=cls_input_index, \n",
    "                                            output_index=cls_output_index,\n",
    "                                            output_shape=cls_output_shape,\n",
    "                                            )\n",
    "        \n",
    "        #If you would want mean over all the softmax output from the frames\n",
    "        y_pred_mean.append(output.mean(axis=0).tolist())\n",
    "    \n",
    "    write_to_output(output_path, file_list, y_pred_mean, taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    TARGET_SR = 8000 #sys.argv[1]\n",
    "    n_mels = 64 #sys.argv[2]\n",
    "    mel_hop_len = 160 #sys.argv[3]\n",
    "    n_fft = 1024 #sys.argv[4]\n",
    "    hop_size = 0.1 \n",
    "    fmax = None\n",
    "    TEST_DIR = os.path.dirname(os.path.realpath('__file__'))\n",
    "    \n",
    "    # Get the Data path (path to audio files or just one .wav file)\n",
    "    try: \n",
    "        data_path = sys.argv[5]\n",
    "        if os.path.isdir(data_path):\n",
    "            data_path = glob.glob(data_path + '/*.wav')\n",
    "        else:\n",
    "            data_path = [data_path]\n",
    "    except:\n",
    "        TEST_AUDIO_DIR = os.path.join(TEST_DIR, 'data/ust_test')\n",
    "        data_path = [random.choice(glob.glob(TEST_AUDIO_DIR + '/*.wav'))]\n",
    "        \n",
    "    # Get the upstream and classifier path from argv or hardcode the path in else\n",
    "    if len(sys.argv) > 6:\n",
    "        upstream_path = sys.argv[6]\n",
    "        try:\n",
    "            classifier_path = sys.argv[7]\n",
    "        except:\n",
    "            print('Input the path to the classifier also!')\n",
    "            exit(0)\n",
    "    else:\n",
    "        TFLITE_MODELS_DIR = os.path.join(TEST_DIR, 'tflite_models')\n",
    "        upstream_path = os.path.join(TFLITE_MODELS_DIR, 'quantized_default_int8.tflite')\n",
    "        classifier_path = os.path.join(TFLITE_MODELS_DIR, 'mlp_ust.tflite')\n",
    "        \n",
    "    output_dir = os.path.join(TEST_DIR, 'output/sonyc_ust/rpi_test')\n",
    "    output_path = os.path.join(output_dir, 'output_mean.csv')\n",
    "        \n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    \n",
    "    yaml_path = os.path.join(TEST_DIR, 'data/dcase-ust-taxonomy.yaml')\n",
    "\n",
    "    with open(yaml_path) as f:\n",
    "        taxonomy = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    # This function does the following 3 tasks:\n",
    "    # 1. Get the embeddings from the upstream model\n",
    "    # 2. Get the softmax output from the classifier\n",
    "    # 3. Get the mean of softmax for all the frames of an audio sample and write to output_path\n",
    "    process_files(data_path, \n",
    "                  taxonomy, \n",
    "                  output_path, \n",
    "                  upstream_path=upstream_path,\n",
    "                  classifier_path=classifier_path,\n",
    "                  hop_size=hop_size, \n",
    "                  n_mels=n_mels, \n",
    "                  n_fft=n_fft, \n",
    "                  mel_hop_len=mel_hop_len,\n",
    "                  fmax=fmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
